# -*- coding: utf-8 -*-
"""Book Recommender System | Machine Learning Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fW8eGS8jhAD_Xyw6J4cvYlIda8NNjfLn

**Recommender System | Machine Learning**
---A recommender system is a machine learning-based algorithm that uses data to suggest items to users that they might be interested in. Recommender systems are used by many online platforms, including social media, e-commerce, and media streaming sites.
"""

import pandas as pd
import numpy as np

books = pd.read_csv("/content/Books.csv")
users = pd.read_csv("/content/Users.csv")
ratings = pd.read_csv("/content/Ratings.csv")

books.head()

users.head()

ratings.head(200)

books.shape

users.shape

ratings.shape

"""**We will see missing value in books,users and ratings**"""

books.isnull().sum()

users.isnull().sum()

ratings.isnull().sum()

"""**Now we will se duplicate item in books , users , ratings**"""

books.duplicated().sum()

users.duplicated().sum()

ratings.duplicated().sum()

""" **# We will going to build Popularity Based Recommender System
[TOP 50 Highest Average Rating Books] Atleast minimun 250votes on each book.**
"""

books

ratings

"""**We will merge both dataset books,ratings based on ISBN**"""

ratings.merge(books, on='ISBN')

ratings.merge(books,on='ISBN').shape

"""**Store into variable**"""

ratings_with_name = ratings.merge(books,on='ISBN')

ratings_with_name

"""**Now we will find average rating on each books but atleast minimun 250votes on each book.**

**Now we will group by on Book-Title so know that how many votes on each book.**
"""

ratings_with_name.groupby('Book-Title').count()

ratings_with_name.groupby('Book-Title').count()['Book-Rating']

ratings_with_name.groupby('Book-Title').count()['Book-Rating'].reset_index()

"""**Here Book Ratings means How many Number of Ratings on Those book. We will convert into NumberOfRating using rename function**"""

num_rating_df = ratings_with_name.groupby('Book-Title').count()['Book-Rating'].reset_index()
num_rating_df.rename(columns={'Book-Rating':'num_ratings'},inplace = True)

num_rating_df.head(40)

""" **Now we will create similar dataframe in which first column [name of books] and
second column [average Rating of those Books]    AVERAGE = Addition of all Ratings/ Total No. of rating**
"""

avg_rating_df = ratings_with_name.groupby('Book-Title')['Book-Rating'].mean(numeric_only=True).reset_index() # set numeric_only to True to avoid errors from non-numeric values
avg_rating_df.rename(columns={'Book-Rating':'avg_rating'},inplace = True)

avg_rating_df

""" **Now we will merge both dataframe [num_rating_df] and [avg_rating_df] based on
[Book-Title] column**
"""

num_rating_df.merge(avg_rating_df,on='Book-Title')

"""**Store into variable popular_df**"""

popular_df = num_rating_df.merge(avg_rating_df,on = 'Book-Title')

popular_df

"""**Now we will find those books whose NumberOfVotes(Num_rating) is greater than 250**"""

popular_df[popular_df['num_ratings']>=250]

"""**Now we will sort the AverageRating on descending order**"""

popular_df[popular_df['num_ratings']>=250].sort_values('avg_rating',ascending = False)

"""**Now we will find Top 50 books**"""

popular_df[popular_df['num_ratings']>=250].sort_values('avg_rating',ascending = False).head(50)

"""**Store into variable**"""

popular_df = popular_df[popular_df['num_ratings']>=250].sort_values('avg_rating',ascending = False).head(50)

popular_df



"""**Now we will achieve columns of  ['Book-Title', 'Book-Author', 'Image-URL-M','Num_ratings','Average_Rating']  So we will merge popular_df on books dataset**"""

popular_df.merge(books,on='Book-Title')

popular_df.merge(books,on='Book-Title').shape

"""**Book of Harry potter have multiple Book with different ISBN So we have to remove duplicate on Book-Title**"""

popular_df.merge(books,on='Book-Title').drop_duplicates('Book-Title')

popular_df.merge(books,on='Book-Title').drop_duplicates('Book-Title').shape

"""**Now we will achieve columns of  ['Book-Title', 'Book-Author', 'Image-URL-M','Num_ratings','Average_Rating']**"""

popular_df.merge(books,on='Book-Title').drop_duplicates('Book-Title')[['Book-Title', 'Book-Author', 'Image-URL-M','num_ratings','avg_rating']]
# Use a list of column names enclosed in square brackets to select multiple columns

popular_df = popular_df.merge(books,on='Book-Title').drop_duplicates('Book-Title')[['Book-Title', 'Book-Author', 'Image-URL-M','num_ratings','avg_rating']]
# Use a list of column names enclosed in square brackets to select multiple columns

popular_df

popular_df['Image-URL-M'][0]

"""////////////////////////////////////////////////////////////////////////////////

#Collaborative Filtering Based Recommender System
"""

ratings_with_name = ratings.merge(books,on='ISBN')

ratings_with_name

"""**we will find those user who has minimun 250 votes on books in a way Experienced User and we will consider those books have minimum 50 votes on each book which is Popular books**

we will find those user who has minimun 250 votes on books in a way Experienced User.
"""

ratings_with_name.groupby('User-ID').count()

"""**In below table User and how many rating gives on books**"""

ratings_with_name.groupby('User-ID').count()['Book-Rating']

"""**we will find those user who has minimun 200 votes on books in a way Experienced User.**"""

ratings_with_name.groupby('User-ID').count()['Book-Rating']>200

x = ratings_with_name.groupby('User-ID').count()['Book-Rating']>200
x[x]

x[x].index #apply index function

"""***ABOVE /Total 811 users where each user who 200 votes on book ***"""

x = ratings_with_name.groupby('User-ID').count()['Book-Rating']>200
x[x].index

"""**Store into variable**"""

Experienced_users = x[x].index

ratings_with_name

"""**Now we will put variable x in ratings_with_name dataframe So that we can find result of all data of Those users who vote minimum 250 votes on book.**

"""

ratings_with_name['User-ID'].isin(Experienced_users)

ratings_with_name[ratings_with_name['User-ID'].isin(Experienced_users)]

"""**Store into Variable**"""

filtered_rating = ratings_with_name[ratings_with_name['User-ID'].isin(Experienced_users)]

"""**Now we will filtering based on book. those books have minimum 50 votes on each book which is Popular books**"""

filtered_rating.groupby('Book-Title').count()['Book-Rating']>=50

y = filtered_rating.groupby('Book-Title').count()['Book-Rating']>=50

"""**This is book who have Rating >=50**"""

y[y]

y[y].index

"""**Store into variable**"""

famous_books = y[y].index

famous_books

"""***Now we will put varibale famous_books in filtered_rating ***"""

filtered_rating['Book-Title'].isin(famous_books)

filtered_rating[filtered_rating['Book-Title'].isin(famous_books)]

"""**Store into variable**"""

final_ratings = filtered_rating[filtered_rating['Book-Title'].isin(famous_books)]

final_ratings

final_ratings.drop_duplicates() #NO duplicate

final_ratings.pivot_table(index='Book-Title',columns='User-ID',values='Book-Rating')

"""**Store into variable**"""

pt = final_ratings.pivot_table(index='Book-Title',columns='User-ID',values='Book-Rating')

pt

pt.fillna(0,inplace= True)

pt

"""**To find the similarity among all vectors to each vector using Euclidean Methods**"""

from sklearn.metrics.pairwise import cosine_similarity

"""***we need to find cosine simlarity of all rows ***"""

cosine_similarity(pt)

"""**Store into variable**"""

similarity_scores = cosine_similarity(pt)

"""**Every book has to have similarity with every book.**"""

similarity_scores.shape

"""**We will need to create function RECOMENDE in which we will give name of book then those function will suggest me name of 5 books**"""

def recommend(book_name):
  return suggestions

"""**We need to fetch index number from book_name**"""

np.where(pt.index=='1984') [0][0]

np.where(pt.index=='4 Blondes') [0][0]

np.where(pt.index=='Zoya') [0][0]

def recommend(book_name):
  index = np.where(pt.index==book_name) [0][0]
  distnaces = similarity_scores[index]
  return suggestions



"""**Now we will find similar item of first book (1984)**"""

similarity_scores[0]

"""**We will find most similar**"""

list(enumerate(similarity_scores[0]))

sorted(list(enumerate(similarity_scores[0])))  # sorted based on first column(index number)

"""Above shell book(1984)  similarity score is (0, 1.0000000000000002), 1984 similarity score is (1, 0.10822627660790704) and so on.

**Sorted based on second column(similarity_scores) which we needed**
"""

sorted(list(enumerate(similarity_scores[0])),key=lambda x:x[1],reverse = True)

# sorted(list(enumerate(similarity_scores[0])),key=lambda x:x[0],reverse = True)

"""**We will find 1 to 6 books from list**"""

sorted(list(enumerate(similarity_scores[0])),key=lambda x:x[1],reverse = True)[1:6]

def recommend(book_name):
  index = np.where(pt.index==book_name)[0][0]
  similar_items = sorted(list(enumerate(similarity_scores[index])),key=lambda x:x[1],reverse = True)[1:6]

  for i in similar_items:
    print(pt.index[i[0]])
  # return suggestions

recommend('Message in a Bottle')

recommend('The Notebook')

recommend('1984')

pt.index

pt.index[530]

pt.index[551]

"""**In below code same as above but in this we want to achieve Author name and image also**"""

def recommend(book_name):
  index = np.where(pt.index==book_name)[0][0]
  similar_items = sorted(list(enumerate(similarity_scores[index])),key=lambda x:x[1],reverse = True)[1:6]

  data = []
  for i in similar_items:
      item = []
      temp_df = books[books['Book-Title'] == pt.index[i[0]]]
      item.extend(list(temp_df.drop_duplicates('Book-Title')['Book-Title'].values))
      item.extend(list(temp_df.drop_duplicates('Book-Title')['Book-Author'].values))
      item.extend(list(temp_df.drop_duplicates('Book-Title')['Image-URL-M'].values))

      data.append(item)
  return data

recommend('Harry Potter and the Chamber of Secrets (Book 2)')

"""
```

"""

pt.index[545]

recommend('1984')

"""**In Above (def recommend) code function use  pt,similarity score, books. so we have to Export these three things(pt,similarity score, books)**"""

import pickle
pickle.dump(pt,open('pt.pkl','wb'))
pickle.dump(books,open('books.pkl','wb'))
pickle.dump(similarity_scores,open('similarity_scores.pkl','wb'))

"""**In popular_df we have to detailed of all book show in web**"""

popular_df

"""**Export popular_df file and get on vscode**"""

import pickle

pickle.dump(popular_df,open('popular.pkl','wb'))